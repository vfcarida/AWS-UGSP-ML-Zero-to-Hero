{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Web App de Análise de Sentimento\n",
    "## Usando PyTorch e SageMaker\n",
    "\n",
    "_Esse tutorial foi feito com base no projeto final do Nanodegree em Deep Learning da Udacity_\n",
    "\n",
    "---\n",
    "\n",
    "Agora que vocês já conhecem um pouco de AWS e de Sagemaker, iremos construir um projeto completo (end to end) com o intuito de desenvolver um web app onde o usuário insira um texto de avaliação de filme e saiba se essa avaliação é positiva ou negativa.\n",
    "\n",
    "## Linhas Gerais\n",
    "\n",
    "\n",
    "Esboço geral para projetos SageMaker usando uma instância de notebook.\n",
    "\n",
    "1. Baixe ou recupere os dados.\n",
    "2. Processe / prepare os dados.\n",
    "3. Faça upload dos dados processados para S3.\n",
    "4. Treine um modelo escolhido.\n",
    "5. Teste o modelo treinado (normalmente usando um trabalho de transformação em lote).\n",
    "6. Implante o modelo treinado.\n",
    "7. Use o modelo implantado.\n",
    "\n",
    "Para este projeto, seguiremos todas as etapas gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.76)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.76 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.76)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.76->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.76->boto3>=1.14.12->sagemaker==1.72.0) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=e148b841145c44ca84a4b4c0559d270b36709a512f06072e33dd7eacb98666bb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.41.0\n",
      "    Uninstalling sagemaker-2.41.0:\n",
      "      Successfully uninstalled sagemaker-2.41.0\n",
      "Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Baixando os dados\n",
    "\n",
    "Para o nosso projeto, usaremos uma base bastante conhecida chamada [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-31 23:46:19--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  8.31MB/s    in 13s     \n",
      "\n",
      "2021-05-31 23:46:33 (6.09 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Preparando e processando os dados\n",
    "\n",
    "Processaremos os nossos dados a fim de deixá-los em um formato mais fácil para realizar o treinamento do modelo. Primeiramente, iremos unir as avaliação positivas e negativas em uma mesma estrutura de dados, uma vez que os dados vem em arquivos separados. Após isso iremos separá-los em treino e teste, garantindo que eles estejam misturados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que lemos os dados brutos de treinamento e teste do conjunto de dados baixado, combinaremos as avaliações positivas e negativas e embaralharemos os registros resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos dar uma olhadinha nos nossos dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyone else who has commented negatively about this film have done excellent analysis as to why this film is so bloody awful. I wasn't going to comment, but the film just bugs me so much, and the writer/director in particular. So I must toss in my hat to join the naysayers.<br /><br />I saw the original \"Wicker Man\" and really loved the cornucopia of music, sensuality, paganism in a modern world, and the clash of theological beliefs. This said, I am not part of the crowd that thinks remakes of great movies shouldn't be done. For example, I liked the original 1950's \"Invasion of the Body Snatchers\", but equally enjoyed the 1978 remake. Both films can stand on their own. Another example is \"The Thing\". The original, as campy as it looks compared to today's standards, has a lot to be proud of in the 1982 remake with Kurt Russell (my all time favorite horror movie). So that small minority of people who like \"The Wicker Man\" re-make can not accuse me of dissing this piece of crap just because it's a re-make.<br /><br />This film solidified for me Neil LaBute's sexism and misogynistic tendencies. It also made me wonder how executives, wanting to make a serious thriller, would green light a product that is so anti-female. There are too many scenes of Cage hitting women just because he's frustrated with them thwarting his investigation of a missing girl. would he react like this off the island in other cases where suspects aren't forthcoming? The original created a society in which men and women are equal participants in a Goddess based religion. The threat to the main character came from everyone, male and female. There was no sexual hierarchy.<br /><br />The metaphor of bees, drones etc was a bit heavy handed and convenient (\"The drone must die!\"), especially when Cage's character has bee allergies. I kept wondering why the men on the island didn't fight back and use mere physicality to stop these women from treating them like grunts. These were not women with special supernatural powers, and half of them seemed to be pregnant, the other half old and fat, and the rest girls and thin blonde waifs, so if the men really wanted to escape they could do what most men do when they hate women. Physically dominate them. There didn't seem to be any guns or weapons beyond cutting tools to hold them if they were unhappy. But if they were content being drones, why make them unable to speak? They could be used as a threat to Cage because they will defend the community. They are drones because Neil LaBute seems to believe that a society ran by women would leave men castrated. (That movie was made already. \"The Stepford Wives\" anyone?) Classic symptoms from men who are afraid of what may happen if women got their sh*t together and were truly equal citizens.<br /><br />The problem with the man-hating female society is that it makes uninteresting movie viewing and creates unintentional humor when Cage starts knocking women out. I belief LaBute should've left the society an egalitarian one, kept the sexuality and uninhibited lasciviousness, and pushed buttons of discomfort in regards to the children on that island. No one likes pedophiles or children to be sexually exploited. So how would a cop react if he saw lewd acts performed by adults with children around? There would be a logical mental leap that these children are abused, thus, an urgency created to save the missing child and get help for all the children. LaBute has said he created the fiancé and daughter story thread to give Cage's character an incentive to search. I don't think you need that. Any child abused will make an adult react to save them. The irony of course would be that the child Cage \"saves\" ultimately brings him death.<br /><br />The dialogue was contrived and campy. The whole third act was hilarious. The audience I saw it with guffawed (and later booed at the end). I just thought the movie started off wrong when the letter arrived written in the fancy handwriting and all the flashbacks cutting into to show how wounded Cage is. We don't need that. Just show him arriving on the island for an investigation of a missing child. Most of us in America have seen \"Law & Order\" and other cop procedurals. We come into the movie as if we are Cage's partner solving a mystery.<br /><br />So much potential...wasted. Neil LaBute, stick to talking head pictures for people who enjoy your male angst-ridden plays and flicks of that sort. Stay with your own company of men. Leave the thrillers for people who understand thrillers. Here is your jar of honey. I'll watch that.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primeiro passo de pré processamento, iremos limpar as tags HTML que podem aparecer nas avaliações e, após isso, iremos _tokenizar_ nossos dados para que palavras como *entertained* e *entertaining* sejam consideradas iguais no nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `review_to_words` utiliza a biblioteca `BeautifulSoup` para remover as tags HTML e usa a biblioteca `nltk` para tokenizar as avaliações e remover as stopwords.  \n",
    "Abaixo, podemos ver o output dessa função aplicado a uma avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everyon',\n",
       " 'els',\n",
       " 'comment',\n",
       " 'neg',\n",
       " 'film',\n",
       " 'done',\n",
       " 'excel',\n",
       " 'analysi',\n",
       " 'film',\n",
       " 'bloodi',\n",
       " 'aw',\n",
       " 'go',\n",
       " 'comment',\n",
       " 'film',\n",
       " 'bug',\n",
       " 'much',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'particular',\n",
       " 'must',\n",
       " 'toss',\n",
       " 'hat',\n",
       " 'join',\n",
       " 'naysay',\n",
       " 'saw',\n",
       " 'origin',\n",
       " 'wicker',\n",
       " 'man',\n",
       " 'realli',\n",
       " 'love',\n",
       " 'cornucopia',\n",
       " 'music',\n",
       " 'sensual',\n",
       " 'pagan',\n",
       " 'modern',\n",
       " 'world',\n",
       " 'clash',\n",
       " 'theolog',\n",
       " 'belief',\n",
       " 'said',\n",
       " 'part',\n",
       " 'crowd',\n",
       " 'think',\n",
       " 'remak',\n",
       " 'great',\n",
       " 'movi',\n",
       " 'done',\n",
       " 'exampl',\n",
       " 'like',\n",
       " 'origin',\n",
       " '1950',\n",
       " 'invas',\n",
       " 'bodi',\n",
       " 'snatcher',\n",
       " 'equal',\n",
       " 'enjoy',\n",
       " '1978',\n",
       " 'remak',\n",
       " 'film',\n",
       " 'stand',\n",
       " 'anoth',\n",
       " 'exampl',\n",
       " 'thing',\n",
       " 'origin',\n",
       " 'campi',\n",
       " 'look',\n",
       " 'compar',\n",
       " 'today',\n",
       " 'standard',\n",
       " 'lot',\n",
       " 'proud',\n",
       " '1982',\n",
       " 'remak',\n",
       " 'kurt',\n",
       " 'russel',\n",
       " 'time',\n",
       " 'favorit',\n",
       " 'horror',\n",
       " 'movi',\n",
       " 'small',\n",
       " 'minor',\n",
       " 'peopl',\n",
       " 'like',\n",
       " 'wicker',\n",
       " 'man',\n",
       " 'make',\n",
       " 'accus',\n",
       " 'diss',\n",
       " 'piec',\n",
       " 'crap',\n",
       " 'make',\n",
       " 'film',\n",
       " 'solidifi',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'sexism',\n",
       " 'misogynist',\n",
       " 'tendenc',\n",
       " 'also',\n",
       " 'made',\n",
       " 'wonder',\n",
       " 'execut',\n",
       " 'want',\n",
       " 'make',\n",
       " 'seriou',\n",
       " 'thriller',\n",
       " 'would',\n",
       " 'green',\n",
       " 'light',\n",
       " 'product',\n",
       " 'anti',\n",
       " 'femal',\n",
       " 'mani',\n",
       " 'scene',\n",
       " 'cage',\n",
       " 'hit',\n",
       " 'women',\n",
       " 'frustrat',\n",
       " 'thwart',\n",
       " 'investig',\n",
       " 'miss',\n",
       " 'girl',\n",
       " 'would',\n",
       " 'react',\n",
       " 'like',\n",
       " 'island',\n",
       " 'case',\n",
       " 'suspect',\n",
       " 'forthcom',\n",
       " 'origin',\n",
       " 'creat',\n",
       " 'societi',\n",
       " 'men',\n",
       " 'women',\n",
       " 'equal',\n",
       " 'particip',\n",
       " 'goddess',\n",
       " 'base',\n",
       " 'religion',\n",
       " 'threat',\n",
       " 'main',\n",
       " 'charact',\n",
       " 'came',\n",
       " 'everyon',\n",
       " 'male',\n",
       " 'femal',\n",
       " 'sexual',\n",
       " 'hierarchi',\n",
       " 'metaphor',\n",
       " 'bee',\n",
       " 'drone',\n",
       " 'etc',\n",
       " 'bit',\n",
       " 'heavi',\n",
       " 'hand',\n",
       " 'conveni',\n",
       " 'drone',\n",
       " 'must',\n",
       " 'die',\n",
       " 'especi',\n",
       " 'cage',\n",
       " 'charact',\n",
       " 'bee',\n",
       " 'allergi',\n",
       " 'kept',\n",
       " 'wonder',\n",
       " 'men',\n",
       " 'island',\n",
       " 'fight',\n",
       " 'back',\n",
       " 'use',\n",
       " 'mere',\n",
       " 'physic',\n",
       " 'stop',\n",
       " 'women',\n",
       " 'treat',\n",
       " 'like',\n",
       " 'grunt',\n",
       " 'women',\n",
       " 'special',\n",
       " 'supernatur',\n",
       " 'power',\n",
       " 'half',\n",
       " 'seem',\n",
       " 'pregnant',\n",
       " 'half',\n",
       " 'old',\n",
       " 'fat',\n",
       " 'rest',\n",
       " 'girl',\n",
       " 'thin',\n",
       " 'blond',\n",
       " 'waif',\n",
       " 'men',\n",
       " 'realli',\n",
       " 'want',\n",
       " 'escap',\n",
       " 'could',\n",
       " 'men',\n",
       " 'hate',\n",
       " 'women',\n",
       " 'physic',\n",
       " 'domin',\n",
       " 'seem',\n",
       " 'gun',\n",
       " 'weapon',\n",
       " 'beyond',\n",
       " 'cut',\n",
       " 'tool',\n",
       " 'hold',\n",
       " 'unhappi',\n",
       " 'content',\n",
       " 'drone',\n",
       " 'make',\n",
       " 'unabl',\n",
       " 'speak',\n",
       " 'could',\n",
       " 'use',\n",
       " 'threat',\n",
       " 'cage',\n",
       " 'defend',\n",
       " 'commun',\n",
       " 'drone',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'seem',\n",
       " 'believ',\n",
       " 'societi',\n",
       " 'ran',\n",
       " 'women',\n",
       " 'would',\n",
       " 'leav',\n",
       " 'men',\n",
       " 'castrat',\n",
       " 'movi',\n",
       " 'made',\n",
       " 'alreadi',\n",
       " 'stepford',\n",
       " 'wive',\n",
       " 'anyon',\n",
       " 'classic',\n",
       " 'symptom',\n",
       " 'men',\n",
       " 'afraid',\n",
       " 'may',\n",
       " 'happen',\n",
       " 'women',\n",
       " 'got',\n",
       " 'sh',\n",
       " 'togeth',\n",
       " 'truli',\n",
       " 'equal',\n",
       " 'citizen',\n",
       " 'problem',\n",
       " 'man',\n",
       " 'hate',\n",
       " 'femal',\n",
       " 'societi',\n",
       " 'make',\n",
       " 'uninterest',\n",
       " 'movi',\n",
       " 'view',\n",
       " 'creat',\n",
       " 'unintent',\n",
       " 'humor',\n",
       " 'cage',\n",
       " 'start',\n",
       " 'knock',\n",
       " 'women',\n",
       " 'belief',\n",
       " 'labut',\n",
       " 'left',\n",
       " 'societi',\n",
       " 'egalitarian',\n",
       " 'one',\n",
       " 'kept',\n",
       " 'sexual',\n",
       " 'uninhibit',\n",
       " 'lascivi',\n",
       " 'push',\n",
       " 'button',\n",
       " 'discomfort',\n",
       " 'regard',\n",
       " 'children',\n",
       " 'island',\n",
       " 'one',\n",
       " 'like',\n",
       " 'pedophil',\n",
       " 'children',\n",
       " 'sexual',\n",
       " 'exploit',\n",
       " 'would',\n",
       " 'cop',\n",
       " 'react',\n",
       " 'saw',\n",
       " 'lewd',\n",
       " 'act',\n",
       " 'perform',\n",
       " 'adult',\n",
       " 'children',\n",
       " 'around',\n",
       " 'would',\n",
       " 'logic',\n",
       " 'mental',\n",
       " 'leap',\n",
       " 'children',\n",
       " 'abus',\n",
       " 'thu',\n",
       " 'urgenc',\n",
       " 'creat',\n",
       " 'save',\n",
       " 'miss',\n",
       " 'child',\n",
       " 'get',\n",
       " 'help',\n",
       " 'children',\n",
       " 'labut',\n",
       " 'said',\n",
       " 'creat',\n",
       " 'fianc',\n",
       " 'daughter',\n",
       " 'stori',\n",
       " 'thread',\n",
       " 'give',\n",
       " 'cage',\n",
       " 'charact',\n",
       " 'incent',\n",
       " 'search',\n",
       " 'think',\n",
       " 'need',\n",
       " 'child',\n",
       " 'abus',\n",
       " 'make',\n",
       " 'adult',\n",
       " 'react',\n",
       " 'save',\n",
       " 'ironi',\n",
       " 'cours',\n",
       " 'would',\n",
       " 'child',\n",
       " 'cage',\n",
       " 'save',\n",
       " 'ultim',\n",
       " 'bring',\n",
       " 'death',\n",
       " 'dialogu',\n",
       " 'contriv',\n",
       " 'campi',\n",
       " 'whole',\n",
       " 'third',\n",
       " 'act',\n",
       " 'hilari',\n",
       " 'audienc',\n",
       " 'saw',\n",
       " 'guffaw',\n",
       " 'later',\n",
       " 'boo',\n",
       " 'end',\n",
       " 'thought',\n",
       " 'movi',\n",
       " 'start',\n",
       " 'wrong',\n",
       " 'letter',\n",
       " 'arriv',\n",
       " 'written',\n",
       " 'fanci',\n",
       " 'handwrit',\n",
       " 'flashback',\n",
       " 'cut',\n",
       " 'show',\n",
       " 'wound',\n",
       " 'cage',\n",
       " 'need',\n",
       " 'show',\n",
       " 'arriv',\n",
       " 'island',\n",
       " 'investig',\n",
       " 'miss',\n",
       " 'child',\n",
       " 'us',\n",
       " 'america',\n",
       " 'seen',\n",
       " 'law',\n",
       " 'order',\n",
       " 'cop',\n",
       " 'procedur',\n",
       " 'come',\n",
       " 'movi',\n",
       " 'cage',\n",
       " 'partner',\n",
       " 'solv',\n",
       " 'mysteri',\n",
       " 'much',\n",
       " 'potenti',\n",
       " 'wast',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'stick',\n",
       " 'talk',\n",
       " 'head',\n",
       " 'pictur',\n",
       " 'peopl',\n",
       " 'enjoy',\n",
       " 'male',\n",
       " 'angst',\n",
       " 'ridden',\n",
       " 'play',\n",
       " 'flick',\n",
       " 'sort',\n",
       " 'stay',\n",
       " 'compani',\n",
       " 'men',\n",
       " 'leav',\n",
       " 'thriller',\n",
       " 'peopl',\n",
       " 'understand',\n",
       " 'thriller',\n",
       " 'jar',\n",
       " 'honey',\n",
       " 'watch']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a função abaixo, `preprocess_data`, aplica a função `review_to_words` para cada uma das avaliações dos datasets de treino e teste. Além disso, ela também faz o cache dos dados, para que, caso algo aconteça, você possa voltar o pré processamento de onde parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando os dados\n",
    "\n",
    "Agora, nós iremos construir uma representação dos nossos dados muito similar a representação conhecida como bag-of-words. Para a rede neural recorrente que iremos usar, nós iremos relizar a transformação dos dados da seguinte forma:\n",
    "\n",
    "1. Transformar cada palavra em um número inteiro;\n",
    "2. Definir um tamanho para nosso vocabulário, ou seja, iremos remover palavras que aparecem pouco (para essas palavras atribuiremos o mesmo número inteiro (1));\n",
    "3. Como estamos usando uma RNN, defineros um tamanho para nossas sequência, ou seja, truncaremos aquelas que forem maiores e iremos inserir um caractér (0) para quando a avaliação for menor do que o tamanho definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começaremos construindo uma função que nos retorna um dicionário de tamanho especifíco e com as palavras que mais aparecem. Não podemos esquecer de reservar o índice 0 e 1 para os caractéres vazio e pouco frequente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for review in data:\n",
    "        for word in review:\n",
    "            if word_count.get(word) is None:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "    sorted_words = [k for k,v in sorted(word_count.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando nosso dicionário\n",
    "\n",
    "Mais para frente, quando tivermos nosso modelo, teremos que usar nosso dicionário para realizar as predições. Sendo assim, precisamos salvá-lo para usar no futuro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando as avaliações\n",
    "\n",
    "Agora, é hora de convertermos nossas avaliações de treino e teste para a sequência de número inteiros de tamanho fixo que entrará na nossa rede neural!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas para validar se nossa função está fazendo tudo corretamente, vamos verificar um dado da nossa base de treino e ver seu tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 226  265  329 1287    3  143  222 3939    3 1559  299   25  329    3\n",
      " 1694   21  308   67  765  130 2932 1735 1352    1  135   90    1   55\n",
      "   16   29    1   85 4638    1  583   92 4147    1 1713  233   63 1611\n",
      "   30  822   26    2  143  360    5   90 1483 3514  458    1  866   77\n",
      " 4322  822    3  390   79  360   35   90 2253   19  509  433  695   70\n",
      " 2320    1  822 2734 1955    6  395  102    2  340 1079   23    5    1\n",
      "   55    8 2166    1  267  522    8    3    1 2928    1    1    1 4291\n",
      "   27   34  108  826   50    8  566  529   15 1134  359  217 1104  544\n",
      "   46   18 1491  382  305 1479    1 1170  240   89   15 2547    5  915\n",
      "  326 1030    1   90  331  795  283  305  866 2221    1  341 1808 2752\n",
      "  216    9  333  226  774  544  556    1 2843    1 4974  453  124 1052\n",
      "  228 2981 4974  130  262  187 1491    9    1    1  749  108  283  915\n",
      "  257   64   65 1001 1014  368  305  775    5    1  305  221 2135  271\n",
      "  252   39 2418  252   72 1701  302   89 1383 1318    1  283   16   50\n",
      "  640   36  283  427  305 1014 1734   39  648 1549  652  374 2776  521\n",
      " 3539 1260 4974    8 1889  482   36   65 2752 1491 2248  980 4974 2928\n",
      "    1   39   94  795 1938  305   15  219  283    1    2   34  404    1\n",
      " 3466  181  242    1  283 1463  121  109  305  111 3541  223  317  866\n",
      " 2204  204   55  427  544  795    8 2111    2  234  331 1913  351 1491\n",
      "   87 1578  305 1713    1  245  795    1    4  749  556    1    1 1235\n",
      " 2891    1 1111  375  915    4    5    1  375  556 1070   15  602 2547\n",
      "  135    1   32   60  642  375  110   15 1304  997 3300  375 1297 1246\n",
      "    1  331  319  240  422   10  163  375    1  233  331 2799  423   13\n",
      " 3170   57 1491    9    1 1032   30  122  422 1297    8  642 2547  319\n",
      " 2554  190   15  422 1491  319  732  324  248  338 1774 2253  144  721\n",
      "   32  499  179  135    1  231 4826   22   99    2   87  294 1965 1038\n",
      "  343 2772    1 1250  374   20 1915 1491  122   20 1038  915 1170  240\n",
      "  422   98  762   47  903  471  602    1   45    2 1491 1432 1972  397\n",
      "   21  790  224 2928    1  860  241  253  269   23   77  774 4518 3940\n",
      "   33  342  330  441  918  283  219  529   23  244  529 3092    1   12\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "500\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(train_X[100])\n",
    "print(len(train_X[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Subindo nossos dados de treino para o S3\n",
    "\n",
    "Nós iremos precisar subir nossos dados de treino para o S3 para que possamos acessá-lo durante o treinamento.\n",
    "\n",
    "### Salvando os dados de treino localmente\n",
    "\n",
    "Antes de subir para o S3, iremos salvar nossos dados localmente. É muito importante saber a estrutura dos dados que vamos salvar, para que possamos utilizar de forma correta. No nosso caso, as linhas do nosso dataset irão ter a forma (colunas): `label`, `length`, `review[500]`, onde `review[500]`é a sequência com 500 números inteiros que geramos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subindo para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:** A célula acima sobe todos os arquivos contidos no nosso diretório para o S3. Isso inclui o `word_dict.pkl`, que usaremos na hora de realizar uma nova predição e garantir que o pré processamento dos novos dados seja o mesmo dos dados de treino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 4: Construindo e treinando nosso modelo em Pytorch\n",
    "\n",
    "\n",
    "Em particular, um modelo compreende três objetos\n",
    "\n",
    " - Artefatos de modelo,\n",
    " - Código de Treinamento e\n",
    " - Código de inferência,\n",
    " \n",
    "cada um dos quais interage um com o outro. Implementaremos nossa própria rede neural no PyTorch junto com um script de treinamento. Para os fins deste projeto, fornecemos o objeto de modelo necessário no arquivo `model.py`, dentro da pasta `train`. Você pode ver a implementação fornecida executando a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conclusão importante da implementação fornecida é que existem três parâmetros que podemos desejar ajustar para melhorar o desempenho de nosso modelo. Estas são a dimensão de embedding, a dimensão oculta e o tamanho do vocabulário. Provavelmente, desejaremos tornar esses parâmetros configuráveis no script de treinamento para que, se desejarmos modificá-los, não precisemos modificar o próprio script. Para começar, escreveremos parte do código de treinamento no notebook para que possamos diagnosticar mais facilmente quaisquer problemas que surjam.\n",
    "\n",
    "Primeiro, carregaremos uma pequena parte do conjunto de dados de treinamento para usar como amostra. Seria muito demorado tentar treinar o modelo completamente no notebook, pois não temos acesso a uma gpu e a instância de computação que estamos usando não é particularmente poderosa. No entanto, podemos trabalhar com alguns dados para ter uma ideia de como nosso script de treinamento está se comportando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Escrevendo o método de treinamento\n",
    "\n",
    "Em seguida, precisamos escrever o código de treinamento. Isso deve ser muito semelhante aos métodos de treinamento que escrevemos antes para treinar modelos em PyTorch. Vamos deixar todos os aspectos difíceis, como salvar / carregar o modelo e carregar os parâmetros, para um pouco mais tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(batch_X)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supondo que temos o método de treinamento acima, testaremos se ele está funcionando escrevendo um pouco de código no notebook que executa nosso método de treinamento no pequeno conjunto de treinamento de amostra que carregamos anteriormente. A razão para fazer isso no notebook é para que tenhamos a oportunidade de corrigir quaisquer erros que surjam no início, quando são mais fáceis de diagnosticar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6934759020805359\n",
      "Epoch: 2, BCELoss: 0.6848492383956909\n",
      "Epoch: 3, BCELoss: 0.6776467561721802\n",
      "Epoch: 4, BCELoss: 0.6698715329170227\n",
      "Epoch: 5, BCELoss: 0.6606590151786804\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir um modelo PyTorch usando o SageMaker, devemos fornecer ao SageMaker um script de treinamento. Podemos opcionalmente incluir um diretório que será copiado para o contêiner e a partir do qual nosso código de treinamento será executado. Quando o contêiner de treinamento é executado, ele verifica o diretório carregado (se houver) para um arquivo `requirements.txt` e instala todas as bibliotecas Python necessárias, após o qual o script de treinamento será executado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo\n",
    "\n",
    "Quando um modelo PyTorch é construído no SageMaker, um ponto de entrada deve ser especificado. Este é o arquivo Python que será executado quando o modelo for treinado. Dentro do diretório `train` está um arquivo chamado` train.py` que é fornecido e que contém o código necessário para treinar nosso modelo. \n",
    "\n",
    "A maneira como o SageMaker passa hiperparâmetros para o script de treinamento é por meio de argumentos. Esses argumentos podem ser analisados e usados no script de treinamento. Para ver como isso é feito, dê uma olhada no arquivo `train / train.py` fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c5.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 2,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 00:57:13 Starting - Starting the training job...\n",
      "2021-06-01 00:57:14 Starting - Launching requested ML instances......\n",
      "2021-06-01 00:58:24 Starting - Preparing the instances for training......\n",
      "2021-06-01 00:59:29 Downloading - Downloading input data...\n",
      "2021-06-01 00:59:54 Training - Downloading the training image.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,473 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,476 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,487 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,491 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/38/3f/4c42a98c9ad7d08c16e7d23b2194a0e4f3b2914662da8bc88986e4e6de1f/regex-2021.4.4.tar.gz (693kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/42/d7/f357d98e9b50346bcb6095fe3ad205d8db3174eb5edb03edfe7c4099576d/tqdm-4.61.0-py2.py3-none-any.whl (75kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvgacggc/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/05/a8/b85fa0bd7850b99f9b4f106972975f2e3c46412e12f9949b58\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, regex, joblib, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\n",
      "2021-06-01 01:00:15 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.1 regex-2021.4.4 soupsieve-2.1 tqdm-4.61.0 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:36,022 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:36,035 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"log_level\": 20,\n",
      "    \"module_dir\": \"s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-06-01-00-57-12-739\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"num_cpus\": 4,\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-06-01-00-57-12-739\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 0.6758968538167526\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.589828126284541\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.5041245781645483\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.4260524918838423\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.3772974458276009\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.36206504824210184\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.3235926025984239\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.3144906558552567\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.2952807351034515\u001b[0m\n",
      "\n",
      "2021-06-01 02:34:07 Uploading - Uploading generated training model\n",
      "2021-06-01 02:34:07 Completed - Training job completed\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.2743338230921298\u001b[0m\n",
      "\u001b[34m2021-06-01 02:33:59,485 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 5678\n",
      "Billable seconds: 5678\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 5: Implantando o modelo em um aplicativo da web\n",
    "\n",
    "Agora que sabemos que nosso modelo está funcionando, é hora de criar algum código de inferência personalizado para que possamos enviar ao modelo uma revisão que não foi processada e determinar o sentimento da revisão.\n",
    "\n",
    "Como vimos acima, por padrão, o estimador que criamos, quando implantado, usará o script de entrada e o diretório que fornecemos ao criar o modelo. No entanto, como agora desejamos aceitar uma string como entrada e nosso modelo espera uma revisão processada, precisamos escrever algum código de inferência personalizado.\n",
    "\n",
    "Vamos armazenar o código que escrevemos no diretório `serve`. Fornecido neste diretório está o arquivo `model.py` que usamos para construir nosso modelo, um arquivo` utils.py` que contém as funções de pré-processamento `review_to_words` e` convert_and_pad` que usamos durante o processamento inicial de dados, e `Predict.py`, o arquivo que conterá nosso código de inferência personalizado. Observe também que `requirements.txt` está presente, o que dirá ao SageMaker quais bibliotecas Python são exigidas por nosso código de inferência personalizado.\n",
    "\n",
    "Ao implantar um modelo PyTorch no SageMaker, espera-se que você forneça quatro funções que o contêiner de inferência SageMaker usará.\n",
    " - `model_fn`: Esta função é a mesma função que usamos no script de treinamento e diz ao SageMaker como carregar nosso modelo.\n",
    " - `input_fn`: esta função recebe a entrada serializada bruta que foi enviada para o endpoint do modelo e seu trabalho é desserializar e disponibilizar a entrada para o código de inferência.\n",
    " - `output_fn`: esta função pega a saída do código de inferência e seu trabalho é serializar esta saída e retorná-la ao chamador do endpoint do modelo.\n",
    " - `Predict_fn`: O coração do script de inferência, é aqui que a previsão real é feita e é a função que você precisa completar.\n",
    "\n",
    "Para o site simples que estamos construindo durante este projeto, os métodos `input_fn` e` output_fn` são relativamente diretos. Só exigimos ser capazes de aceitar uma string como entrada e esperamos retornar um único valor como saída. Você pode imaginar, entretanto, que em um aplicativo mais complexo, a entrada ou saída podem ser dados de imagem ou alguns outros dados binários que exigiriam algum esforço para serializar.\n",
    "\n",
    "\n",
    "### Escrevendo código de inferência\n",
    "\n",
    "Começaremos dando uma olhada no código que foi fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.word_dict = pickle.load(f)\n",
      "\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\n",
      "\n",
      "    data_X = \u001b[34mNone\u001b[39;49;00m\n",
      "    data_len = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\n",
      "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\n",
      "    data_pack = np.hstack((data_len, data_X))\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    data = torch.from_numpy(data_pack)\n",
      "    data = data.to(device)\n",
      "\n",
      "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\n",
      "    model.eval()\n",
      "\n",
      "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\n",
      "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\n",
      "\n",
      "    result = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mencionado anteriormente, o método `model_fn` é o mesmo fornecido no código de treinamento e os métodos` input_fn` e `output_fn` são muito simples e sua tarefa será completar o método` predict_fn`. Certifique-se de salvar o arquivo completo como `predict.py` no diretório` serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implantando o modelo\n",
    "\n",
    "Agora que o código de inferência personalizado foi escrito, criaremos e implantaremos nosso modelo. Para começar, precisamos construir um novo objeto PyTorchModel que aponta para os artefatos do modelo criados durante o treinamento e também aponta para o código de inferência que desejamos usar. Em seguida, podemos chamar o método de implantação para iniciar o contêiner de implantação.\n",
    "\n",
    "** NOTA **: O comportamento padrão para um modelo PyTorch implantado é assumir que qualquer entrada passada ao preditor é um array `numpy`. Em nosso caso, queremos enviar uma string, então precisamos construir um wrapper simples em torno da classe `RealTimePredictor` para acomodar strings simples. Em uma situação mais complicada, você pode desejar fornecer um objeto de serialização, por exemplo, se desejar enviar dados de imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo\n",
    "\n",
    "Agora que implantamos nosso modelo com o código de inferência personalizado, devemos testar para ver se tudo está funcionando. Aqui, testamos nosso modelo carregando os primeiros `250` comentários positivos e negativos e os enviamos para o endpoint, em seguida, coletamos os resultados. O motivo para enviar apenas alguns dos dados é que o tempo que leva para nosso modelo processar a entrada e, em seguida, realizar a inferência é muito longo e, portanto, testar todo o conjunto de dados seria proibitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                results.append(float(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos que nosso endpoint está funcionando conforme o esperado, podemos configurar a página da web que irá interagir com ele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 6: Criando nossa função Lambda\n",
    "\n",
    "Construimos nosso endpoint e deployamos nosso modelo, mas... e se quisessemos que nosso modelo fosse acessável por meio de um webapp? Para isso, precisamos construir alguns outros componentes, que podemos ver na arquitetura abaixo:\n",
    "\n",
    "<img src=\"../deploy-aws.png\">\n",
    "\n",
    "Indo da esquerda para direita temos: \n",
    "\n",
    "- A EC2 será responsável por subir nossa aplicação Flask, servir nossa página e enviar a requisição para o API Gateway;\n",
    "- O API Gateway receberá a requisição e encaminhará para a função Lambda;\n",
    "- Já a função Lambda funcionará como redirecionando o texto da nova avaliação para o Sagemaker endpoint, uma vez que não há como realizar a integração API Gateway -> Sagemaker Endpoint de forma nativa e também não podemos chamar a URL do Sagemaker Endpoint sem estar dentro da AWS.\n",
    "\n",
    "### Configurando nossa função Lambda\n",
    "\n",
    "A primeira coisa que faremos será a criação da nossa função Lambda. Nossa função receberá como input os dados vindos do nosso API Gateway, irá realizar a chamada do endpoint e retornará a resposta para o API Gateway.\n",
    "\n",
    "#### Parte A: Criando IAM Role para a Lambda\n",
    "\n",
    "Já que queremos que nossa função Lambda chame o Sagemaker Endpoint, precisamos garantir que ele tenha permissão para isso. Dessa forma, precisamos adicionar essa permissão dentro da Role que usaremos em nossa função Lambda.\n",
    "\n",
    "Usando o Console da AWS, procuraremos por **IAM** na barra de busca e clicaremos em **Roles** no menu esquerdo. Feito isso, clique em **Create Role**. Garante que em **AWS service** o time de _trusted entity_ selecionado seja **Lambda** e, em seguida, clique em **Next: Permissions**.\n",
    "\n",
    "Na barra de busca procure por `sagemaker` e clique no checkbox referente a **AmazonSageMakerFullAccess** policy. Clique em **Next: Review**.\n",
    "\n",
    "Por último, dê um nome para usa role e garanta que você irá se lembrar na hora de criar sua função Lambda!\n",
    "\n",
    "Usaremos o nome `LambdaSageMakerRole`.\n",
    "\n",
    "#### Parte B: Criando a Lambda\n",
    "\n",
    "Agora, é hora de criarmos nossa função Lambda!\n",
    "\n",
    "Para isso, no console AWS, procure por Lambda e clique em **Create a function**. Na página seguinte, clique em **Author from scratch**, selecione o runtime como sendo Python, dê um nome para sua função, como por exemplo: `sentiment_analysis_func`. Não esqueça de selecionar a role que criamos!\n",
    "\n",
    "Após isso, clique em **Create Function**.\n",
    "\n",
    "Na próxima página você verá algumas informações sobre sua função Lambda que você acabou de criar. Se você descer um pouco a página, verá um editor de texto onde você pode escrever código que será executado quando sua função for chamada. No nosso projeto, usaremos o código abaixo (basta copiar e colar) e não esqueça de mudar o `EndpointName` para o nome do endpoint que pegaremos na célula abaixo.\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2021-06-01-03-42-13-849'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após alterar o nome do endpoint na sua função Lambda, clique em **Salvar**. Feito, sua função Lambda estará pronto para ser executada!\n",
    "\n",
    "## Passo 7: Criando e configurando o API Gateway\n",
    "\n",
    "Agora, é hora de criarmos nosso API Gateway!\n",
    "\n",
    "Abra o console da AWS e busque por **API Gateway** e, na página inicial, procure por **REST API** (vão ter duas, selecione a que NÃO aparece private) e clique em **Build**. Na tela de configuração, na seção **Create new API** selecione **New API** e defina um nome para seu Gateway e clique em **Create API**.\n",
    "\n",
    "Já dentro da sua API, precisamos criar nosso método POST, que receberá o texto a ser predito, e integrá-lo a função Lambda que criamos. Para isso, na aba **Resources** do menu lateral, clicaremos no botão **Actions** e selecionaremos a opção **Create Method**. Na lista dos nossos recursos aparecerá um dropdown vazio, clique nele e selecione **POST** e depois clique no &#x2611;.\n",
    "\n",
    "Na tela de configuração do nosso método POST preencha da seguinte forma:\n",
    "\n",
    "- **Integration type**:  Lambda Function;\n",
    "- **Use Lambda Proxy integration**: marque essa checkbox;\n",
    "- **Lambda Region**: Região que você criou seus outros recursos (no geral, já vem preenchido corretamente);\n",
    "- **Lambda Function**: Nome da função Lambda que criamos no passo anterior.\n",
    "\n",
    "Após preencher, clique em **Save**.\n",
    "\n",
    "Para finalizar, precisamos deployar nosso API Gateway. Para isso, na aba **Resources** do menu lateral, clicamos no botão **Actions** e depois em **Deploy API**. Irá aparecer um pop-up no qual clicaremos em **Deployment stage**, selecionaremos a opção **[New Stage]** e depois daremos um nome a esse estágio (pode ser _prod_). Por fim, clicamos em **Deploy**.\n",
    "\n",
    "Nessa janela que abrirá, você verá na parte superior a URL do seu API Gateway. Salve ela em algum lugar pois usaremos no passo a seguir, quando estivermos configurando nosso webapp!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Deployando nosso web app\n",
    "\n",
    "Agora, como passo final, faremos o deploy do nosso web app, para que possamos acessá-lo pelo browser e saber o sentimento de novas avaliações de filmes!  \n",
    "Para isso, usaremos uma instância de EC2 e subiremos uma aplicação Flask que será responsável por carregar a página e fazer as chamadas no API Gateway.\n",
    "\n",
    "Comece procurando por **EC2** na barra de busca do console AWS. No menu lateral esquerdo clique em **Instances** e procure por um botão laranja, na parte superior direita, com o seguinte texto: **Launch instances**. Clique nele.\n",
    "\n",
    "Após isso, você será redirecionado para uma tela onde terá que selecionar a AMI (imagem do sistema) que você irá utilizar em sua EC2. Selecione **Amazon Linux 2 AMI (HVM), SSD Volume Type** na versão **64-bit (x86)**. Ao clicar em **Select** você será redirecionado para a página onde poderá escolher o tipo da sua instância. Selecioner o tipo **t2.micro** e clique em **Next: Configure Instance Details**.\n",
    "\n",
    "Na página seguinte, não precisa alterar nada, apenas clicar em **Next: Add Storage**. Em seguida, clique em **Next: Add tags** e depois em **Next: Configure Security Group**.\n",
    "\n",
    "Nessa página configurar o `security group` da EC2 para garantir que seja permitido o ingresso pela porta 80 (Adicionar uma inbound rule permitindo HTTP para qualquer Ipv4). Para isso, clicamos em **Add rule**, mudamos o tipo para **HTTP** e o Source para **Anywhere**. Feito isso, clique em **Review and Launch** e depois em **Launch**.\n",
    "\n",
    "Nessa hora, aparecerá um pop-up perdindo para você selecionar um par de chaves para fazer a conexão SSH. Não usaremos, pois iremos nos conectar à máquina via browser. Assim, selecione a opção **Proceed without a key pair**, marque o checkbox e clique em **Launch instances**.\n",
    "\n",
    "Agora, temos que aguardar a máquina ficar com o status de pronta e, quando estiver tudo certo, clicamos em cima da nossa instância e depois em **Connect**.\n",
    "\n",
    "Após isso, vamos acessar, via browser, o terminal da máquina que criamos e instalar o git (```yum install git```). Com o git instalado, iremos executar o comando ```https://github.com/vfcarida/AWS-UGSP-ML-Zero-to-Hero```. Não esqueça de, após clonar, entrar no diretório (```cd aws-deploy-example```).\n",
    "\n",
    "Feito isso, iremos acessar a pasta `website\\templates` e abrir o arquivo chamado `index.html` (```vim index.html```) para que possamos alterar a URL da nossa API. Procure por uma linha que contém **\\*\\*REPLACE WITH PUBLIC API URL\\*\\*** e insira a url da sua API onde será possível realizar a predição. Feito isso, salve o arquivo (```Esc -> :wq```) e volte para o diretório `website` (```cd ..```)`.\n",
    "\n",
    "Já no repositório `website`, execute o comando ```pip3 install -r requirements.txt``` e, depois disso, o comando ```python3 -m flask run --host 0.0.0.0 --port 80```.\n",
    "\n",
    "Pronto! Basta acessar o Ip público da sua máquina (fica na parte inferior da tela) e ver sua aplicação funcionando!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 9: Deletando a EC2 e o Sagemaker Endpoint\n",
    "\n",
    "Não se esqueça de deletar os recursos assim que finalizar o uso, para que não gere cobranças desnecessárias.\n",
    "\n",
    "Para apagar a EC2 procure por **EC2** na barra de busca do console AWS. No menu lateral esquerdo clique em **Instances** e depois selecione a instância que você criou para o nosso exemplo. Na parte superior, clique em **Instance State** e depois em **Terminate Instance**.\n",
    "\n",
    "Já para o Sagemaker Endpoint, apenas rode o comando da célula abaixo.\n",
    "\n",
    "OBS: Para deletar os outros recursos que criamos (Lambda e API Gateway) basta acessar tais produtos, selecionar o que você quer deletar e clicar em **Actions** e **Delete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
